<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="3D Gaussian Inpainting with Depth-Guided Cross-View Consistency">
  <meta name="keywords" content="Gaussian Splatting, 3D Inpainting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3DGIC</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/script.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
    };
  </script>
  <!-- 2) Load MathJax -->
  <script
    async
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>



</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: #76b900 ;font-weight: bolder;">3D</span> <span style="color: #76b900 ;font-weight: bolder;">G</span>aussian <span style="color: #76b900 ;font-weight: bolder;">I</span>npainting with Depth-Guided <span style="color: #76b900 ;font-weight: bolder;">C</span>ross-view Consistency</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=mzr_nekAAAAJ&hl=zh-TW">Sheng-Yu Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://timchou-ntu.github.io/">Zi-Ting Chou</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Frank Wang</a><sup>1,2</sup></span>
          </div>

          <!-- affiliations -->
          <div class="subtitle is-size-5">
            <sup>1</sup> Graduate Institute of Communication Engineering, National Taiwan University &nbsp;
            <sup>2</sup> Nvidia
          </div>

          <!-- vendor -->
          <div class="subtitle is-size-5">
            CVPR 2025
          </div>

          <!-- TODO: update paper and video link once upload -->
          <div class="column has-text-centered">
            <!-- <div class="publication-links"> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/3dgic"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.11801"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
            </div>

            <!-- <div class="subtitle is-size-5">
            </div>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <div class="content has-text-justified">
                    <strong>Updates</strong>
                    <ul>
                      <li><strong>Mar 18, 2025.</strong> Revise literature review. Support depthanythingv2 relative depth loss and mast3r metric depth loss for a better geometry.</li>
                      <li><strong>Mar 8, 2025.</strong> Support ScanNet++ dataset. Check official <a href="https://kaldir.vc.in.tum.de/scannetpp/benchmark/nvs">benchmark</a> for our results on the 3rd-party hidden set evaluation over 50 indoor scenes. Our <a href="https://github.com/NVlabs/svraster/blob/main/articles/scannetpp_dataset.md">short report</a> may be helpful if you want to work on scannet or indoor environement.</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div> -->

        </div>
      </div>
    </div>
    </div>
    </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class=" carousel results-carousel">
          <div class="item">
            <video autoplay controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_bicycle.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_flowers.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_Courthouse.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_Barn.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_kitchen.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_stump.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fly_through_bonsai.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Given a 3D Gaussian Splatting model
              $G_{1:N}$ pretrained on multi-view images
              $I_{1:K}$ at camera poses
              $\xi_{1:K}$, our goal is to perform 3D inpainting based on
              the object masks $M_{1:K}$ (e.g., provided by SAM). With
              the rendered depth maps $D_{1:K}$, the stage of
              Inferring Depth-Guide Inpainting Mask is able to refine the inpainting
              masks to preserve visible backgrounds across camera views. The stage
              of Inpainting-guided 3DGS Refinement then utilizes such masks to joint-
              ly update the new Gaussians $G'_{1:N'}$ for both novel-view
              rendering and inpainting purposes.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <div class="column is-four-fifths"></div>
      <div class="has-text-centered">
        <img src="./images/main.png">
      </div>
    </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Inferring Depth-Guided Inpainting Mask</h2>
          <div class="content has-text-justified">
            <p>
              Taking $\{I_1, M_1\}$ at view $\xi_1$ as an example reference view, the original background region $I^B_1$ can be first produced. We then project the background region $I^B_2$ from $\xi_2$ to $\xi_1$, updating ${I'}^B_1$ and the associated inpainting mask $M'_1$. By repeating this process across camera views, the final inpainting mask $M'_1$ contains only the regions that are \textit{not} visible at any training camera views.
            </p>
            <div style="max-width: 700px; margin: auto;">
              <img src="./images/mask_reduction.png">
            </div>
            
          </div>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Difference of Inpainting Between Using Original Object Mask and Our Mask</h2>
          <div class="content has-text-justified">
            <p>
              Although the original object mask $M_1$ can be used to inpaint the occluded regions, it may also include some background regions. This leads to a inpainted background conflicting existing one. In contrast, our mask $M'_1$ is more accurate and can preserve visible background contents.
            </p>
            <div style="max-width: 700px; margin: auto;">
              <img src="./images/mask_diff.png">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Inpainting Results</h2>
          <div class="content has-text-justified">
            <div style="max-width: 700px; margin: auto;">
              <img src="./images/quantitative_nvs.jpg">
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example1" class="bal-container-small">
            <div class="bal-after">
              <video
                id="video-ours"
                loop muted playsinline preload="auto"
                style="width:100%; display:block;">
                <source src="./videos/spinnerf_2_ours.mp4" type="video/mp4">
              </video>
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            
            <div class="bal-before" style="width:96.4968%;">
              <div class="bal-before-inset" style="width:512px;">
                <video
                  id="video-original"
                  loop muted playsinline preload="auto"
                  style="width:100%; display:block;">
                  <source src="./videos/spinnerf_2_gt.mp4" type="video/mp4">
                </video>
                <div class="bal-beforePosition beforeLabel">
                  Original
                </div>
              </div>
            </div>
            
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example2" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/stump_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/stump_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example3" class="bal-container-small">
            <div class="bal-after">
              <img src="./images/qualitative/playroom_ours.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            <div class="bal-before" style="width:96.4968152866242%;">
              <div class="bal-before-inset" style="width: 512px;">
                <img src="./images/qualitative/playroom_3dgs.png">
                <div class="bal-beforePosition beforeLabel">
                  3DGS
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example4" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/drjohnson_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/drjohnson_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example5" class="bal-container-small">
            <div class="bal-after">
              <img src="./images/qualitative/train_ours.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            <div class="bal-before" style="width:96.4968152866242%;">
              <div class="bal-before-inset" style="width: 512px;">
                <img src="./images/qualitative/train_3dgs.png">
                <div class="bal-beforePosition beforeLabel">
                  3DGS
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example6" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/truck_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/truck_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Adaptive Sparse Voxel Fusion</h2>
          <div class="content has-text-justified">
            <p>
              Fusing 2D modalities into the trained sparse voxels is efficient. The grid points simply take the weighted sum from the 2D views following classical volume fusing method. We show several examples in the following.
            </p>
            <p>
              <strong>Rendered depths &rarr; Sparse-voxel SDF &rarr; Mesh</strong>
            </p>
            <div style="margin: auto;">
              <img src="./images/meshing.jpg">
            </div>
            <br>
            <br>
            <p>
              <strong>Image segmentation by Segformer &rarr; Sparse-voxel semantic field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_segformer.ipynb">here</a>.
            </p>
            <div class="text-columns">
              <div class="text-column">Render view</div>
              <div class="text-column">3D fused semantic field</div>
              <div class="text-column">2D Segformer predction</div>
            </div>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_segformer_bicycle.mp4"
                      type="video/mp4">
            </video>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_segformer_room.mp4"
                      type="video/mp4">
            </video>
            <br>
            <br>
            <p>
              <strong>Vision foundation model feature by RADIOv2.5 &rarr; Voxel pooling &rarr; Sparse-voxel foudation feature field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_vfm_radio.ipynb">here</a>.
            </p>
            <div class="text-columns">
              <div class="text-column">Render view</div>
              <div class="text-column">3D fused RADIO feature field</div>
              <div class="text-column">2D RADIO predction</div>
            </div>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_radio_bonsai.mp4"
                      type="video/mp4">
            </video>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_radio_garden.mp4"
                      type="video/mp4">
            </video>
            <br>
            <br>
            <p>
              <strong>Dense CLIP feature by LangSplat &rarr; Voxel pooling &rarr; Sparse-voxel language field</strong>
            </p>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_langfeat.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related work</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Data structure</strong>
              <ul>
                <li><a href="https://research.nvidia.com/sites/default/files/pubs/2010-02_Efficient-Sparse-Voxel/laine2010tr1_paper.pdf"><u>Sparse Voxel Octrees</u></a> uses an Octree to manage the sparse voxels. </li>
                <li><a href="https://www.museth.org/Ken/Publications_files/Museth_TOG13.pdf"><u>VDB</u></a> uses a shallow tree with wide branching factor to manage the sparse volumes. Extensions include <a href="https://www.museth.org/Ken/Publications_files/Museth_SIG14.pdf">OpenVDB</a>, <a href="https://research.nvidia.com/labs/prl/nanovdb/nanovdb2021.pdf">NanoVDB</a>, <a href="https://arxiv.org/pdf/2208.04448">NeuralVDB</a>, and the recent <a href="https://arxiv.org/pdf/2407.01781">fVDB</a></li> 
                <li><u>Our method</u> does not use any advanced data structure.</li>
              </ul>
              We simply store all the non-empty voxels of different size in a 1D array. Our rasterizer ensures everything is correctly ordered when rendering. One limitation is that we do not support efficient search (ie., given a point, find its covering voxel). Future work needs to sort the voxel and implement a binary search for this purpose.
            </p>
            <p>
              <strong>Sparse field</strong>
              <ul>
                <li><u>Global-sparse-local-dense</u> strategy allocates a tiny dense 3D grid to each sparse volume. Classical methods like <a href="https://niessnerlab.org/papers/2013/4hashing/niessner2013hashing.pdf">Voxel hasing</a>, <a href="https://arxiv.org/pdf/1604.01093">Bundle fusion</a>, and <a href="https://www.open3d.org/docs/latest/tutorial/t_reconstruction_system/voxel_block_grid.html#voxel-block-grid">Open3D</a> uses this strategy to do sensor fusion.</li>
                <li><u>Implicit sparse neural field</u> like <a href="https://arxiv.org/pdf/2007.11571">NSVF</a> and <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Takikawa_Neural_Geometric_Level_of_Detail_Real-Time_Rendering_With_Implicit_3D_CVPR_2021_paper.pdf">NGLOD</a> employs MLP network to map the latent feature stored in the Sparse Voxel Octree to the target attributes.</li>
                <li><u>Our method</u> allocate just a single voxel at leaf nodes with explicit density and color parameters, like <a href="https://arxiv.org/pdf/2112.05131">Plenoxels</a>.</li>
              </ul>
            </p>
            <p>
              <strong>Scalability</strong>
              <ul>
                <li><u>Uniform-level</u> sparse voxels allocates voxel to a given target level. <a href="https://arxiv.org/pdf/2007.11571">NSVF</a>, <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Takikawa_Neural_Geometric_Level_of_Detail_Real-Time_Rendering_With_Implicit_3D_CVPR_2021_paper.pdf">NGLOD</a>, <a href="https://kaolin.readthedocs.io/en/v0.9.1/modules/kaolin.ops.spc.html">SPC</a>, <a href="https://arxiv.org/pdf/2112.05131">Plenoxels</a> follow this design, which encouters scalability issue when using higher grid resolution for better quality.</li>
                <li><u>Our method</u> allocates voxel on different Octree levels. We only keep the leaf nodes in a 1D array without advanced data structure.</li>
              </ul>
              For now, our maximum Octree levels is 16 which implies 65536<sup>3</sup> maximum grid resolution. Extention to more Octree level should be simple. The increase in rendering time should be minor: 3 extra bits to sort per additional level.
            </p>
            <p>
              <strong>Novel scene representations</strong>
              <p><a href="https://arxiv.org/pdf/2501.16312">LinPrim</a>, <a href="https://arxiv.org/abs/2502.01157">RadiantFoam</a>, <a href="https://arxiv.org/pdf/2502.07754">MeshSplats</a> are recent methods that use very different and cool approach for reconstruction and novel-view synthesis tasks.</p>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <script>
    new BeforeAfter({
      id: '#example1'
    });
    new BeforeAfter({
      id: '#example2'
    });
    new BeforeAfter({
      id: '#example3'
    });
    new BeforeAfter({
      id: '#example4'
    });
    new BeforeAfter({
      id: '#example5'
    });
    new BeforeAfter({
      id: '#example6'
    });
  </script>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
<code>@article{Sun2024SVR,
  title={Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering},
  author={Cheng Sun and Jaesung Choe and Charles Loop and Wei-Chiu Ma and Yu-Chiang Frank Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.04459},
}</code>
      </pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!-- <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a> -->
        <!-- <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i> -->
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              We thank the authors of
              We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the
              template of this website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const v1 = document.getElementById('video-ours');
      const v2 = document.getElementById('video-original');
    
      // When both videos can play through, reset to 0 and start together
      let readyCount = 0;
      [v1, v2].forEach(v => {
        v.addEventListener('canplaythrough', () => {
          if (++readyCount === 2) {
            v1.currentTime = v2.currentTime = 0;
            Promise.all([v1.play(), v2.play()])
              .catch(() => {
                console.warn('Auto-play was prevented; user interaction may be required.');
              });
          }
        });
      });
    
      // (Optional) lock them at the same playback rate
      v1.playbackRate = v2.playbackRate = 1.0;
    });
    </script>
</body>

</html>